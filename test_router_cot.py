#!/usr/bin/env python3
"""
Script de test pour valider le routeur avec Chain of Thought.

Ce script teste les 3 cas d'usage de r√©f√©rence :
1. "g√©n√®re un site e-commerce de chaussures de luxe"
2. "d√©compose FIR-3 en user stories"
3. "quelle heure est-il ?"

Pour chaque test, on v√©rifie :
- La pr√©sence du log [ROUTER_THOUGHT]
- La coh√©rence du raisonnement
- La correspondance entre l'agent s√©lectionn√© et la cha√Æne de pens√©e
"""

import json
import os
import sys
from pathlib import Path

# Ajouter le r√©pertoire parent au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent))

from dotenv import load_dotenv
from litellm import completion

from agent4ba.ai.graph import load_router_prompt
from agent4ba.ai.schemas import RouterDecision
from agent4ba.core.logger import setup_logger

# Charger les variables d'environnement
load_dotenv()

# Configurer le logger
logger = setup_logger(__name__)


def test_router_decision(test_case: str, expected_agent: str) -> bool:
    """
    Teste une d√©cision du routeur.

    Args:
        test_case: La t√¢che √† router
        expected_agent: L'agent attendu

    Returns:
        True si le test r√©ussit, False sinon
    """
    print(f"\n{'=' * 80}")
    print(f"TEST: {test_case}")
    print(f"{'=' * 80}")

    # Charger le prompt
    prompt_config = load_router_prompt()

    # Pr√©parer le prompt utilisateur
    user_prompt = prompt_config["user_prompt_template"].replace(
        "{{ rewritten_task }}", test_case
    )

    # R√©cup√©rer le mod√®le depuis l'environnement
    model = os.getenv("DEFAULT_LLM_MODEL", "gpt-4o-mini")
    temperature = float(os.getenv("LLM_TEMPERATURE", "0.0"))

    try:
        # Appeler le LLM
        response = completion(
            model=model,
            messages=[
                {"role": "system", "content": prompt_config["system_prompt"]},
                {"role": "user", "content": user_prompt},
            ],
            temperature=temperature,
        )

        # Extraire la r√©ponse
        routing_json_str = response.choices[0].message.content.strip()

        print(f"\nüìù R√©ponse brute du LLM:")
        print(routing_json_str)

        # Parser le JSON dans un objet RouterDecision
        routing_data = json.loads(routing_json_str)
        router_decision = RouterDecision(**routing_data)

        # Valider la structure
        router_decision.validate_decision()

        # Afficher la cha√Æne de pens√©e (simulation du log [ROUTER_THOUGHT])
        print(f"\nüß† [ROUTER_THOUGHT] {router_decision.thought}")

        # Extraire les √©l√©ments de la d√©cision
        agent_id = router_decision.decision.get("agent")
        agent_task = router_decision.decision.get("task")
        args = router_decision.decision.get("args", {})

        print(f"\n‚úÖ Agent s√©lectionn√©: {agent_id}")
        print(f"‚úÖ T√¢che s√©lectionn√©e: {agent_task}")
        print(f"‚úÖ Arguments: {json.dumps(args, indent=2, ensure_ascii=False)}")

        # V√©rifier si l'agent correspond √† l'attendu
        if agent_id == expected_agent:
            print(f"\n‚úÖ TEST R√âUSSI: Agent attendu '{expected_agent}' correctement s√©lectionn√©")
            return True
        else:
            print(f"\n‚ùå TEST √âCHOU√â: Agent attendu '{expected_agent}', obtenu '{agent_id}'")
            return False

    except json.JSONDecodeError as e:
        print(f"\n‚ùå Erreur de parsing JSON: {e}")
        return False
    except (KeyError, ValueError) as e:
        print(f"\n‚ùå Erreur de validation RouterDecision: {e}")
        return False
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue: {e}")
        return False


def main():
    """Ex√©cute tous les tests."""
    print("\n" + "=" * 80)
    print("üß™ TESTS DU ROUTEUR AVEC CHAIN OF THOUGHT")
    print("=" * 80)

    # Liste des cas de test
    test_cases = [
        {
            "description": "Cr√©ation d'un projet e-commerce from scratch",
            "task": "G√©n√®re un site e-commerce de chaussures de luxe",
            "expected_agent": "epic_architect_agent",
        },
        {
            "description": "D√©composition d'une feature existante",
            "task": "D√©compose FIR-3 en user stories",
            "expected_agent": "story_teller_agent",
        },
        {
            "description": "Requ√™te hors-scope (fallback)",
            "task": "Quelle heure est-il ?",
            "expected_agent": "fallback_agent",
        },
    ]

    # Ex√©cuter les tests
    results = []
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n\n{'#' * 80}")
        print(f"# TEST {i}/3: {test_case['description']}")
        print(f"{'#' * 80}")

        success = test_router_decision(
            test_case["task"],
            test_case["expected_agent"]
        )
        results.append({
            "test": test_case["description"],
            "success": success
        })

    # Afficher le r√©sum√©
    print("\n\n" + "=" * 80)
    print("üìä R√âSUM√â DES TESTS")
    print("=" * 80)

    total = len(results)
    passed = sum(1 for r in results if r["success"])
    failed = total - passed

    for i, result in enumerate(results, 1):
        status = "‚úÖ R√âUSSI" if result["success"] else "‚ùå √âCHOU√â"
        print(f"{i}. {result['test']}: {status}")

    print(f"\n{'=' * 80}")
    print(f"TOTAL: {passed}/{total} tests r√©ussis ({failed} √©checs)")
    print(f"{'=' * 80}\n")

    # Retourner un code de sortie appropri√©
    sys.exit(0 if failed == 0 else 1)


if __name__ == "__main__":
    main()
